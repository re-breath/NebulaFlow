#rebreath的函数库
#环境变量部分的函数
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
echo "#env setting"> ~/.rebreath/.config
source ~/.rebreath/.config
export PATH=$PATH:$HOME/.rebreath
alias loadenv='source ~/.bashrc'
alias modenv='vim ~/.bashrc'
alias py='python3'
export PYTHONPATH=$PYTHONPATH:$HOME/.rebreath/nebula_pylib
module load compiler/gcc/12.2.0 2> /dev/null || true
alias modrenv='vim ~/.rebreath/rebreath-env-function'

#处理数据集数据的快捷键，可以使用来处理dump文件的数据
#修改显示的样式
#PS1='\[\033[01;36m\]\u——>NebulaFlow:\W\[\033[01;32m\]$\[\033[01;34m\] '


loadxl(){
    PS1='\[\033[01;36m\]xl——>NebulaFlow:\W\[\033[01;32m\]$\[\033[01;34m\] '
}

gpumdstart_dcu(){
#本程序建立是为了简化在曙光上使用gpumd的流程，该程序默认任务名为执行命令的文件夹的名字，默认使用1dcu与1cpu,也可以使用-n参数指定使用dcu的数量
dcu_num=1
for arg in "$@"; do 
   if [[ $arg = "-n" ]]; then
     dcu_num=${2:-1}
     break
   fi
done
gpumd_version=${gpumd_version:"GPUMD"}
job_name=$(basename "$PWD")

echo "#!/bin/bash
#SBATCH -p xahdnormal
#SBATCH -N  1
#SBATCH --ntasks-per-node=$dcu_num
#SBATCH --gres=dcu:$dcu_num
#SBATCH --time 144:00:00
#SBATCH --comment=GPUMD
#SBATCH -o $PWD/std.out.%j
#SBATCH -e $PWD/std.err.%j

# MARK_CMD
source /work/home/rebreath/sbatch_need/gpumd_env.sh
/work/share/acmtrwrxv5/${gpumd_version}/src/gpumd" | sbatch -J "$job_name"
EOF
}



start_gpumd(){
#该函数用来启动gpumd的计算，对于dcu可以进行制定核数的计算,可以使用来代替gpumd的启动
    if [ $gpumd_exe -eq "gpumd" ]; then
        gpumd

    elif [ $gpumd_exe -eq "gpumdstart_dcu" ]; then
        dcu_num=${1:-1}
        gpumdstart_dcu -n $dcu_num
    else
        echo "错误：未知的 gpumd 启动方式，清修改 ~/.rebreath/.config 文件中的配置。"
        exit 521
    fi
}

get_energy(){
#获得xyz文件中的所有的能量
#grep "attice" $xyz_file |  grep -oE '\b\w+="[^"]+"|\b\w+=[^ ]+\b' |grep 'energy'
    local xyz_file=${1:-'dump.xyz'}
    grep "attice" $xyz_file |  grep -oE '\b\w+="[^"]+"|\b\w+=[^ ]+\b' |grep -oE '[Ee]nergy.*' |grep -oE '[-]?[0-9]+[\.]?[0-9]+'
}

get_Lattice(){
#得到xyz文件中所有的晶格参数
    local xyz_file=${1:-'dump.xyz'}
    grep "attice" $xyz_file |  grep -oE '\b\w+="[^"]+"|\b\w+=[^ ]+\b' | grep "attice"
}
get_virial(){
    local xyz_file=${1:-'dump.xyz'}
    grep "attice" $xyz_file |  grep -oE '\b\w+="[^"]+"|\b\w+=[^ ]+\b' | grep "irial"
}
get_configs_num(){ 
#获得构型的数量
    local xyz_file=${1:-'train.xyz'}
    grep "attice" $1 |wc -l
}

get_V(){
#对xyz文件的体积进行计算，并输出为1列
#能够识别两种类型的文件，主要是特定的xyz文件与thermo.out文件
    local xyz_file=${1:-'dump.xyz'}
    if [[ $xyz_file =~ ".xyz" ]];then
    get_Lattice $xyz_file |sed 's/"/ /g' | awk '{printf "%.15f\n",$2*$6*$10}'
    elif [[ $xyz_file =~ ".out" ]];then
        python3 << EOF
import numpy as np
filename='$xyz_file'
data = np.loadtxt(filename)
def get_volume(data):
    lx = data[:,-3]
    ly = data[:,-2]
    lz = data[:,-1]
    return lx*ly*lz
V = get_volume(data)
for i in V:
    print(i)
EOF
    else
        echo "错误：未知的输入文件类型，请检查文件名后重试。"
fi
}

get_col_average(){
#对文件的某一列进行平均，并输出为1列
#使用的方式为 get_col_average thermo.out 10 1 20 表示从thermo.out文件中取第10列，索引1到索引20的平均值,注意最大值最小值接受的为数列切片的最大最小值
#使用案例2：get_col_average thermo.out 10 1
    local filename=$1
    local col_index=$2
    local start_index=$3
    local end_index=$4
    local flag=0
    if (( $# > 4 )); then
        echo "错误：参数过多，请检查命令。"
        return 1
    fi
    if [ -z $3 ]; then
        start_index=0
    fi
    if [ -z $4 ]; then
        end_index=None
    fi
    python3 << EOF
import numpy as np
filename = '$filename'
col_index = int($col_index) - 1
min_index = int($start_index)
max_index = $end_index

data = np.loadtxt(filename)
if data.ndim == 1:
    data = data.reshape(-1, 1)

data = data[:, col_index]
def get_col_average(data, min_index, max_index):
    data = data[min_index:max_index]
    return np.mean(data)

average_value = get_col_average(data, min_index, max_index)

print(f"average file : {filename}\nThe column : {col_index+1}\nRange : [{min_index}:{max_index}]\nAverage value : {average_value:.8f}")
EOF
}

average_file(){
#将输入文件进行平均,使用C++实现，平均后输入到average.out文件中
    local cpplib="$HOME/.rebreath/cpp_lib"
    first_file="$1"
    avg_name=${1:0:4}
    g++ $cpplib/averagefiles.cpp -o average_file
    ./average_file "$@" 
    rm -f average_file   
}

average_file_s(){
#将输入文件进行平均,使用shell实现
    files_num="$#"
    tmp_file='temp'
    first_file="$1"
    avg_name=${1:0:4}
    paste "$@" >$tmp_file
    # 逐行读取处理，无论多少行都能处理
    while IFS= read -r line
    do
        echo "$line" | awk -v files_num="$files_num" '{
            sum=0;
            for(i=1; i<=NF/files_num; i++){
                sum=0;
                for(j=0; j<files_num; j++){
                    sum += $(i+j*NF/files_num)
                }
                printf "%.15f\t", sum/files_num;
            }
            printf "\n";
        }'
    done < "$tmp_file" >average_${avg_name}.out
    rm -f "$tmp_file"
}
average_file_c(){
#将输入文件进行平均,使用C++实现，平均后输入到average.out文件中,后将文件改名
    local cpplib="$HOME/.rebreath/cpp_lib"
    first_file="$1"
    avg_name=${1:0:4}
    g++ $cpplib/averagefiles.cpp -o average_file
    ./average_file "$@" 
    rm -f average_file
    mv average.out average_${avg_name}.out
}


check_hnemd_thermo(){
#检查hnemd的thermo输出文件，并输出平均值,输出最后的晶格参数
    average_file_s thermo_*
    echo $PWD
    echo Lattice :
    tail -n 1 average_ther.out | awk '{print $10,$11,$12}'
}

use_agent(){
    export http_proxy="http://192.168.1.10:3128"
}

replot() {
#该函数将指定文件的前两列进行画图
    if [ "$#" -ne 1 ]; then
        echo "用法: replot <数据文件>"
        return 1
    fi

    python3 <<EOF
import matplotlib.pyplot as plt
import sys

datafile = "$1"

x, y = [], []
with open(datafile, 'r') as file:
    for line in file:
        parts = line.strip().split()
        if len(parts) < 2:
            continue  # 跳过不正确的行
        x_val, y_val = map(float, parts[:2])
        x.append(x_val)
        y.append(y_val)

plt.plot(x, y, marker='o', linestyle='-')
plt.xlabel('X')
plt.ylabel('Y')
plt.savefig('plot.png', format='png')
EOF
}

proxy_download() {
#该函数的作用为使用代理节点下载文件
    local remote_server="dhk@10.14.0.15" 
    local remote_path="/home/dhk/rebreath/proxy_downfiles" 
    local remote_temp_path="${remote_path}/temp" 

    # 用ssh在远程服务器的临时目录创建目录并下载文件
    ssh -i ~/.ssh/id_rsa ${remote_server} "mkdir -p ${remote_temp_path} && cd ${remote_temp_path} && $1"
    
    # 检查远程下载命令的返回值
    if [[ "$?" -ne 0 ]]; then
        echo "文件下载失败，请检查命令并重试。"
        return 1
    fi
    
    # 使用scp从临时文件夹复制所有文件到本地当前目录下
    scp -rv -i ~/.ssh/id_rsa ${remote_server}:${remote_temp_path}/* .
    
    # 检查scp命令的返回值
    if [[ "$?" -ne 0 ]]; then
        echo "文件复制失败，请检查远程服务器上的路径和文件名。"
        return 1
    fi

    # 清空远程服务器中的临时目录
    ssh -i ~/.ssh/id_rsa ${remote_server} "rm -rf ${remote_temp_path}/*"

    echo "文件已成功下载到本地并清理了临时文件夹。"
}

#关闭vasp组件未使用警告
export OMPI_MCA_btl_base_warn_component_unused=0

free_time_run() {
#监测到空闲的gpu后进行任务
  while true; do
    for gpu_id in $(nvidia-smi --query-gpu=index --format=csv,noheader,nounits); do
      mem_used=$(nvidia-smi --id=$gpu_id --query-gpu=memory.used --format=csv,noheader,nounits)
      if [ $mem_used -lt 200 ]; then
          export CUDA_VISIBLE_DEVICES=$gpu_id
          echo "Running task on GPU $gpu_id"
          eval $1
          break 2
      fi
      done
      echo "No free GPU found, waiting..."
      sleep 60
  done
  date '+%Y-%m-%d %H:%M:%S' >> run_train-file.log
  echo -e "执行 $1 \n" >> run_train-file.log    
}

search_large_files() {
# 搜索当前目录下所有大于20GB的文件
    find $1 -type f -size +20G
}
nohup_free_time_run(){
#该函数将任务放到后台运行，并监测到空闲的gpu后进行任务
#新增加的函数，等待检验中（）
    nohup free_time_run "$1" 2>&1 &
}

source $HOME/.rebreath/re_function.sh

#调用ovito转换文件的格式
xyz_to_poscar() {
    python3 -c "from ovito.io import import_file, export_file; pipeline = import_file('$1'); export_file(pipeline, 'POSCAR_convered', 'vasp')"
}

poscar_to_xyz() {     
       	python3 -c "from ovito.io import import_file, export_file; pipeline = import_file('$1'); export_file(pipeline, 'model_conversed.xyz', 'xyz',columns=['Particle Type', 'Position.X', 'Position.Y', 'Position.Z'])"
}


plot_stress_strain_curve(){
#自动检测应变的轴，输出数据到文件中，并进行简单的画图
    source $HOME/.rebreath/plot_library/stress_strain_curve.sh  
}

plot_mul_stress_strain_curve(){
#该函数为自动检测多个deform_{xyz}+文件并自动将其数据进行处理画出图来
    source $HOME/.rebreath/plot_library/auto_plot_xyz_strain_stress_curve.sh
}


get_area_of_xy_and_volume() {
# 计算xy的晶面积,并顺便的计算出斜胞的体积(注意文件对象为gpumd风格的xyz文件)
# 输出一个文件 Volume_area_xy.txt, 第一列为斜胞体积，第二列为xy面的面积
# 注意该函数为临时函数，只计算xy平面的面积，为计算CaF2使用ase进行切片后的固定晶面所需，提交时候需要谨慎考虑
    local file=$1
    get_Lattice $1 |awk -F "=" '{print $2}' | sed 's/"//g'  > lattic.log
    python3 << EOF
import numpy as np
filename = 'lattic.log'
data = np.loadtxt(filename)
la = data[:,:3]
lb = data[:,3:6]
lc = data[:,6:9]
def get_area_xy(la,lb,lc):
    """
    计算xy面的面积(使用叉乘法)
    """
    areas = np.zeros(len(la))
    vols = np.zeros(len(la))
    for i in range(len(la)):
        temp = np.cross(la[i],lb[i])
        areas[i] = np.linalg.norm(temp)
        vols[i] = np.dot(lc[i],temp)

    return areas,vols

areas ,vols = get_area_xy(la,lb,lc)

np.savetxt('Volume_area_xy.txt',np.column_stack((vols,areas)))
EOF
}

xyz_to_cssr() {
# 内嵌的Python脚本
    python3 - "$1" "$2" << 'EOF'
from ase.io import read, write

# 转换函数
def convert(file_in, file_out):
    # 读取XYZ文件
    atoms = read(file_in)
    
    # 写出CSSR文件
    write(file_out, atoms, format='cssr')

# 主函数
if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print("用法: convert_xyz_to_cssr <input.xyz> <output.cssr>")
        sys.return(1)

    # 执行转换
    convert(sys.argv[1], sys.argv[2])

EOF
}

xyz_to_cif () 
#该函数可能会出问题
{ 
    python3 - "$1" "$2" <<'EOF'
from ase.io import read, write

def convert(file_in, file_out):
    # 读取XYZ文件
    atoms = read(file_in)
    
    # 输出为CIF文件
    write(file_out, atoms)

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print("Usage: python $0 <input.xyz> <output.cif>")
        sys.return(1)
    
    file_in = sys.argv[1]
    file_out = sys.argv[2]
    
    convert(file_in, file_out)
EOF

}

select_xyz_config(){
#选择xyz文件中的某一构型(默认选择最后一个构型)，并输出到一个新的xyz文件中
    local xyz_file=${1:-'train.xyz'}
    local config_elected=${2:-'-1'}
    python3 << EOF
from  ase.io import read, write
atoms = read("$xyz_file",index = ":")
config_elected = int($config_elected)
if not config_elected:
    config_elected = -1
if config_elected == -1:
    write("selected.xyz", [atoms[-1]])
else:
    write("selected.xyz", [atoms[config_elected]])
EOF
}

select_xyz_configs(){
#选择xyz文件中的某一组构型，并输出到一个新的xyz文件中
    local xyz_file=${1:-'train.xyz'}
    local config_start=${2:-'0'}
    local config_end=${3:-'1'}

    python3 << EOF
from  ase.io import read, write
atoms = read("$xyz_file",index = ":")
config_start = int($config_start)
config_end = int($config_end)
write("selected.xyz", atoms[config_start:config_end+1])
EOF
}

grouping_to_xyz(){
#该函数用来对xyz类型的文件进行分组
#使用方式为grouping_to_xyz POSCAR x/y/z 1 1.2 3 表示将POSCAR文件中的原子按照x/y/z方向的距离进行分组，分成三份，按照1:1.2:3的比例进行分组
    python3  ~/.rebreath/deal_data/regrouping.py "$@"

}

plot_crysralinity_fraction(){
# 本脚本的功能为对dump.xyz文件的每一帧进行晶型分析，分析其蜂窝状的原子的个数并画出图形进行可视化
# 输入参数为dump.xyz文件名，以及晶型类型，如plot_crysralinity_fraction FCC dump.xyz 
# 能处理的晶型有：OTHER FCC HCP BCC ICO SC CUBIC_DIAMOND HEX_DIAMOND GRAPHENE
    crystal_type="OTHER FCC HCP BCC ICO SC CUBIC_DIAMOND HEX_DIAMOND GRAPHENE"
    # 输入的晶型
    itype=$1
    argfile=$2
    # 将 itype 转换为大写
    itype_upper=$(echo "$itype" | tr '[:lower:]' '[:upper:]')

    # 检查 itype_upper 是否在 crystal_type 列表中
    if [[ ! $crystal_type =~ (^|[[:space:]])"$itype_upper"($|[[:space:]]) ]]; then
        echo "Error: $itype_upper is not a valid crystal type."
        echo "Valid crystal types are: $crystal_type"
        return 1
    fi

python3 << EOF
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
from ovito.io import import_file
from ovito.modifiers import PolyhedralTemplateMatchingModifier
from ovito.data import *

crystal_type = '$itype_upper'
# 设置matplotlib不显示图形界面
matplotlib.use('Agg')
def export_plot_data(listx,listy,filename:str):
    #该函数接受列表x与列表y，将列表x与列表y写入文件第一列与第二列
    date = np.column_stack((listx,listy))
    #print(date)
    np.savetxt(filename,date,delimiter=' ')
    print('数据已保存至',filename)

def plot_graphene_counts(file_pattern, color, label):
    #计算并绘制每个文件的石墨烯原子计数
    pipeline = import_file(file_pattern, multiple_frames=True)

    ptm_modifier = PolyhedralTemplateMatchingModifier()
    ptm_modifier.structures[PolyhedralTemplateMatchingModifier.Type.$itype_upper].enabled = True
    pipeline.modifiers.append(ptm_modifier)

    graphene_counts = []
    frames = []

    for frame_index in range(pipeline.source.num_frames):
        data = pipeline.compute(frame_index)
        frames.append(frame_index)
        graphene_counts.append(data.attributes['PolyhedralTemplateMatching.counts.$itype_upper'])

    export_plot_data(frames, graphene_counts, f"{label}.txt")
    plt.plot(frames, graphene_counts, 'o-', color=color, label=label,markersize=2)
    


def setup_and_save_plot(file_list):
    #设置图表并保存为PNG文件
    plt.figure(figsize=(10, 5))
    for file_data in file_list:
        plot_graphene_counts(file_data['pattern'], file_data['color'], file_data['label'])

    plt.title('$itype_upper Atom Count Over Time')
    plt.xlabel('Time/ns$^{-2}$')
    plt.ylabel('Count of $itype_upper Atoms')
    plt.grid(True)
    plt.legend(loc='upper right')
    plt.savefig("${itype_upper}_atom_count.png")


file_list = [
    {'pattern': '${argfile}', 'color': '#714882', 'label': '${argfile%.*}_$itype_upper'}
]  # 更多文件可以添加到列表

# 调用函数
setup_and_save_plot(file_list)
EOF
}


suggest_expand_coefficient(){
#对给定的正交的晶胞进行建议，计算出特定的原子个数左右需要进行扩胞的扩胞系数
#使用方法：suggest_expand_coefficient model.xyz 10000,会输出建议的扩胞系数
    local xyz_file=${1:-'model.xyz'}
    local atom_num=${2:-'10000'}
    python3 << EOF
import nebula
config = nebula.read_xyz("$xyz_file")
config = config[0]
init_num = config.atom_num
target_num = int($atom_num)
if init_num >= target_num:
    print("当前晶胞中原子数目已经大于或等于目标数目，不需要扩胞。")

lattice = config.lattice
print("当前晶胞的晶格参数为：",lattice)
lattice_a = lattice[0]
lattice_b = lattice[4]
lattice_c = lattice[8]
print("当前晶胞的a,b,c分别为：",lattice_a,lattice_b,lattice_c)
lattice_all = lattice_a + lattice_b + lattice_c
proportion = [lattice_a/lattice_all, lattice_b/lattice_all, lattice_c/lattice_all]
print("当前晶胞的各个方向的占比为：",proportion)
proportion = [1/i for i in proportion]
print("各个晶胞方向的权重为：",proportion)
proportion = [i/sum(proportion) for i in proportion]

expand_coefficient = target_num/init_num
print("当前晶胞需要扩胞的总系数为：",expand_coefficient)
x_1 = (expand_coefficient/(proportion[0]*proportion[1]*proportion[2]))**(1/3)

proportion_expand = [round(proportion[0]*x_1), round(proportion[1]*x_1), round(proportion[2]*x_1)]
print("当前晶胞需要扩胞的各个方向的系数为：",proportion_expand)
lattice_xyz = [lattice_a*proportion_expand[0], lattice_b*proportion_expand[1], lattice_c*proportion_expand[2]]
last_all_expand_coefficient = proportion_expand[0]*proportion_expand[1]*proportion_expand[2]
print("\n——————>建议结果如下：")
print(f"扩胞系数为：{proportion_expand}   总的扩胞系数为：{last_all_expand_coefficient}")
print("扩胞后的晶格参数为：",lattice_xyz)
print("扩胞后的晶胞中原子数目为：",init_num*last_all_expand_coefficient)
EOF
}

plot_ultimate_nep(){
#画出调色后的nep图
    cp $HOME/.rebreath/plot_library/plot_nep_results_ultimate.py .
    python3 plot_nep_results_ultimate.py
    rm -f plot_nep_results_ultimate.py
}

compute_elastic_moduli(){
#使用calorine计算弹性模量 使用方法  compute_elastic_moduli   nepfile
    local compute_lib=/home/dhk/.rebreath/compute_lib
    local nep_file=${1:-nep.txt}
    sed "s/nepfile/$nep_file/g" $compute_lib/calorine_compute_elastic.py > calorine_compute_elastic.py
    python3 calorine_compute_elastic.py > elastic_calorine.txt
    rm -f calorine_compute_elastic.py 
    cat elastic_calorine.txt
}
plot_nep(){
#画出结果的图
    python3 ~/.rebreath/plot_library/hplt_nep_results.py
}

plot_hnemd(){
#该函数用来画hnemd的图像，自动识别路径中的的_{xyz},以此来对特定方向的hnemd画图
    lib_address="$HOME/.rebreath/plot_library/"
    
    find $PWD -type f -regex '.*kappa.out' | while read -r file;do
      local hnemd_direct=0
      if [[ $file =~ _([xyz]) ]];then 
         hnemd_direct=${BASH_REMATCH[-1]} 
         echo -e '\n——>'"找到hnemd的方向了，该处的hnemd计算的为 $hnemd_direct 方向的热导率" 'O.<'
         else
         echo "没有找到hnemd的驱动力方向，请检查是否按约定命名方式命名 oooooo"
         return 1
      fi
    file_address=$(dirname $file)
    cd $file_address
    echo $PWD
    python3 "$lib_address/plot_hnemd_${hnemd_direct}.py" 2>&1
    cd - >/dev/null
    done
}

plot_mul_hnemd(){
#找到average文件夹，自动识别该文件路径中的方向，对其中的文件进行画图
#注意格式需要为kappa_[0-9]+.out
    local lib_address="$HOME/.rebreath/plot_library/"
    local hnemd_direct=0
    local kappa_num=$(find $PWD -type f -regex '.*kappa_[0-9]+.out' |wc -l)
    find $PWD -type d -regex '.*average_hnemd/kappa' | while read -r workdir;do
        cd $workdir
        if [ -f "average.out" ];then
            cp average.out kappa.out
        fi

        if [[ $workdir =~ _([xyz]) ]];then 
            hnemd_direct=${BASH_REMATCH[-1]} 
            echo -e '\n——>'"找到hnemd的方向了，该处的hnemd计算的为 $hnemd_direct 方向的热导率" 'O.<'
            else
            echo "没有找到hnemd的驱动力方向，请检查是否按约定命名方式命名 oooooo"
            return 1
        fi
        cat $lib_address/plot_hnemd_mul_${hnemd_direct}.py |sed "/for i in range(/c\for i in range(${kappa_num}):" > plot_hnemd_mul_${hnemd_direct}.py
        python3 "plot_hnemd_mul_${hnemd_direct}.py" 2>&1
        cd - >/dev/null
    done
}

deal_hnemd_data(){
#处理数据hnmed计算完成的数据，注意需要符合一定的命名规则
#该函数能够将hnemd_[0-9]+类型的文件夹进行整理，对每个将文件夹中的kappa.out进行平均并将其整合到一张图中
#场地要求：使用该函数的地方需要有很多的hnemd_[0-9]+类型的文件夹
    local lib_address="$HOME/.rebreath/deal_data/"
    source ${lib_address}/deal_hnemd_data.sh 
    #cd /average_hnemd/kappa
    plot_mul_hnemd
}

start_mul_hnemd(){
#该函数用来启动gpumd的hnemd方法来计算热导率，可以进行制定次数的热导率的计算
#使用方法：start_mul_hnemd nep.txt 6 1 将会使用gpumd进行6次热导率的计算，每个计算使用1个核
    local nepfile=${1:-nep.txt}
    local times=${2:-6}
    local times=$((times-1))
    local core_num=${3:-1}
    local init_address=$PWD
    for i in $(seq 0 $times);do
        mkdir -p hnemd_${i}
        cp $nepfile model.xyz run.in hnemd_${i}/
        cd hnemd_${i}
        start_gpumd $core_num
        cd $init_address
    done
}



deal_strain_fluctuation_to_elastic(){
#该函数用来处理应变波动法计算弹性模量得到的thermo文件，计算出其中的弹性模量
#使用方法：deal_strain_fluctuation_to_elastic 1200  # 1200为温度
    cp ~/.rebreath/deal_data/deal_strain_fluctuation.sh .
    bash deal_strain_fluctuation.sh $1 > elastics.txt
    rm -f deal_strain_fluctuation.sh
    cat elastics.txt
}

get_hnemd_data(){
    local init_address=$(pwd)
    local xyz="x y z"
    for i in $xyz;
    do 
      cd hnemd_${i}/average_hnemd/kappa/
      python3 plot_hnemd_mul_${i}.py 
      pwd
      cd $init_address

    done
}

verify_gpumd_result(){
#重新计算验算当前的gpumd算例
    local prediction_nep=$(ls |grep -oE "nep.*\.txt")
    local nepfile=${1:-$prediction_nep}
    local dir=$(basename $PWD)
    local verify_dir="verify_${dir}"
    if [ -d "$verify_dir" ];then
        rm -rf $verify_dir
    fi
    mkdir  $verify_dir
    cp "$nepfile" "$verify_dir"
    cp model.xyz run.in $verify_dir/
}

verify_deform(){
#临时函数，用来验算拉伸曲线的结果
    verify_gpumd_result
    cd verify_*
    sed -i  "/dump_thermo/c\dump_thermo  100" run.in
    sed -i  "/run/c\run 1000000" run.in
    gpumdstart
}


find_column_max(){
#检查文件指定列的最大值 使用方式为 find_column_max 4 thermo.out 4 (检查文件第四列的最大值) 
    if [[ $# -ne 2 ]]; then
    echo "错误：需要提供文件名和列号作为参数。"
    return 1
    fi

    filename=$1
    column=$2

    # 检查文件是否存在且可读
    if [[ ! -f "$filename" || ! -r "$filename" ]]; then
        echo "错误：文件 '$filename' 不存在或不可读。"
        return 1
    fi
    # 验证列号是否为正整数
    if [[ ! $column =~ ^[0-9]+$ ]]; then
        echo "错误：列号 '$column' 必须是正整数。"
        return 1
    fi
    # 使用双引号包含变量，并在 awk 脚本中安全地使用变量
    awk -v col="$column" 'BEGIN {max=0} 
        NR > 0 && ($col > max) {max=$col} 
        END { printf("第 %d 列的最大值为 ：%f\n", col, max) }' "$filename"
}

find_column_abs_max(){
#检查文件指定列的绝对值最大值 使用方式为 find_column_max thermo.out 4 (检查文件第四列的最大值) 
    if [[ $# -ne 2 ]]; then
    echo "错误：需要提供文件名和列号作为参数。"
    return 1
    fi

    filename=$1
    column=$2

    # 检查文件是否存在且可读
    if [[ ! -f "$filename" || ! -r "$filename" ]]; then
        echo "错误：文件 '$filename' 不存在或不可读。"
        return 1
    fi
    # 验证列号是否为正整数
    if [[ ! $column =~ ^[0-9]+$ ]]; then
        echo "错误：列号 '$column' 必须是正整数。"
        return 1
    fi
    # 使用双引号包含变量，并在 awk 脚本中安全地使用变量
    awk -v col="$column" 'BEGIN {max=0} 
        NR > 0 && (sqrt(($col)^2) > sqrt((max)^2)) {max=$col} 
        END { printf("第 %d 列的绝对值最大值为：%f\n", col, max) }' "$filename"
}

cell_expansion(){
#使用gpumd进行扩胞 使用的方式类似 cell_expansion 10 10 1
    local nepfile=${nepfile:='nep.txt'}
    local xyzfile=${xyzfile:='model.xyz'}
    tempfile="temp_$(date +%s%3N)"
    mkdir -p $tempfile
    cp $nepfile $xyzfile  $tempfile/
    cd $tempfile/
    cat >run.in << EOF
replicate $1 $2 $3 
potential       $nepfile
ensemble        nve
time_step       0
dump_exyz       1
run             1
EOF
gpumd > /dev/null 2>&1
if [ -f  "dump.xyz" ];then
   cp dump.xyz  ../expanded.xyz
fi
cd - >/dev/null
rm -rf $tempfile
echo "已经完成 $1  $2  $3 扩胞"
}


relib() {
# rebreath库的管理员   
#示例使用方式 relib -v -m hp pre sh

    lib="$HOME/.rebreath"
    view=0
    multi=0
    args=()

    # 处理参数
    while [ $# -gt 0 ]; do
        case "$1" in
            -v)
                view=1
                ;;
            -m)
                multi=1
                ;;
            *)
                args+=("$1")
                ;;
        esac
        shift
    done

    # 检查是否有实际的查找关键词
    if [ ${#args[@]} -eq 0 ]; then
        echo "请注意提供至少一个查找关键词。"
        exit 521
    fi

    # 用于存储中间结果的文件
    tmpfile=$(mktemp)

    # 初始查找
    find "$lib" -type f -name "*${args[0]}*" > "$tmpfile"

    # 逐层次查找
    for keyword in "${args[@]:1}"; do
        new_tmpfile=$(mktemp)
        while IFS= read -r line; do
            find "$line" -type f -name "*$keyword*" >> "$new_tmpfile"
        done < "$tmpfile"
        mv "$new_tmpfile" "$tmpfile"
    done

    # 执行查找和相应操作
    if [ $view -eq 1 ]; then
        cat "$tmpfile"
    else
        while IFS= read -r line; do
            cp "$line" ./
        done < "$tmpfile"
    fi

    # 移除临时文件
    rm -f "$tmpfile"
}




#+++++++++++++++++++++++++++nep训练集相关+++++++++++++++++++++++++++++++++

screening_reasonable_forces(){
#筛选nep的训练集，将训练集的合理的构型提取出
#使用的方法为 screening_reasonable_forces xyzfile min_force max_force
    local deal_lib="$HOME/.rebreath/deal_data/"
    python3 $deal_lib/elect_rely_force.py $1  $2  $3
}
screening_reasonable_energy(){
#筛选nep的训练集，将训练集的合理的能量提取出
#使用的方法为 screening_reasonable_energy xyzfile min_energy max_energy
    local deal_lib="$HOME/.rebreath/deal_data/"
    python3 $deal_lib/elect_rely_energy.py $1  $2  $3
}

screening_reasonable_virial(){
#筛选nep的训练集，将训练集的合理的位力提取出
#使用的方法为 screening_reasonable_virial xyzfile min_strain max_strain
    local deal_lib="$HOME/.rebreath/deal_data/"
    python3 $deal_lib/elect_rely_virial.py $1  $2  $3
}



#+++++++++++++++++++++++++++vasp计算相关++++++++++++++++++++++++++++++++++

add_kspacing_to_incar(){
#如果kspacing的值不为零，则添加kspacing
#使用的方法为 add_kspacing_to_incar 0.2
    kspacing=${1:=0.2}
    if [ -f "KPOINTS" ];then
    mv KPOINTS KPOINTS_backup
    fi
    if [ -z "$(grep "KSPACING" INCAR)" ];then
        echo "KSPACING = $kspacing " >> INCAR
    else
        sed -i -E "/^KSPACING.*=.*/c\KSPACING = $kspacing" INCAR
    fi
}

check_vasp_complete() {
#检查档期那目录的vasp是否完成了计算，如果没有完成就会运行vasp
    vasp_exe=${1:-'vasp'}
    core_num=${2:-'1'}
    if [ ! -f  "OUTCAR" ];then
        echo "当前目录下没有找到OUTCAR文件，正在生成..."
        mpirun -np $core_num $vasp_exe
    elif [ -z "$(grep "General timing and accounting informations for this job" OUTCAR)" ];then
        echo "$PWD 目录下OUTCAR不完整，开始重新计算"
        mpirun -np $core_num $vasp_exe
    fi
}

run_all_vasp_job(){
#检查目录下所有的train-*文件夹，并运行vasp
    starttime=$(date +%s)
    init_address=$PWD
    core_num=${core_num:=1}
    vasp_exe=${vasp_exe:=vasp}
    startime=$(date +%s)
    kspacing=${kspacing:=0.2}

    for i in $(find $init_address/ -type d -name "train-*" |sort)
    do
    cd $i
    add_kspacing_to_incar $kspacing
    if [ ! -f "OUTCAR" ];then
        echo "文件夹$i下没有找到OUTCAR文件，正在对其进行计算"
        check_vasp_complete
        echo "$i的退出码为$?" >> $init_address/run_train-file.log
    fi
    complete=$(grep "General timing and accounting informations for this job" OUTCAR)
    if [ -z "$complete" ];then
    echo "文件夹$i下的OUTCAR文件不完整，正在重新计算"
    #echo 'SYMPREC=1E-03' >> $i/INCAR
    check_vasp_complete
    echo "$i的退出码为$?" >> $init_address/run_train-file.log
    fi
    cd $init_address
    done
    time_log=$(date)
    echo "完成的时间为:  $time_log"
    endtime=$(date +%s)
    alltime=$((endtime-startime))
    min_time=$((alltime/60))
    echo -e "--------------------->run_train-file.sh 完美完成工作,总共用时$min_time min OVO"
}

load_single_point_energy_dir(){
#检查该目录下所有的POSCAR类的文件，计算其单点能
#要求是poscar文件的目录下面有incar与potcar，另外incar
    orig_dir=$PWD
    find $orig_dir -type f -regex ".*POSCAR.*" |while IFS= read -r i; do
    dname=$(dirname $i)
    cd $dname
    filename=$(basename $i)
    suffix=${filename#POSCAR}
    mkdir -p single_energy/train-$suffix
    cp $dname/POTCAR $dname/INCAR single_energy/train-$suffix
    if [ -f "$dname/KPOINTS" ];then
    cp $dname/KPOINTS  single_energy/train-$suffix
    fi
    cp $i single_energy/train-$suffix/POSCAR
    cd $orig_dir
done
}

deal_outcar_to_train(){
#寻找当前所有的OUTCAR文件，将其整理成train.xyz
#注意其寻找的是所有的OUTCAR文件，因此需要确保当前目录下所有OUTCAR文件都为单点能计算的OUTCAR
    startime=$(date +%s)
    writ_file="train.xyz"
    init_address=$PWD
    if [ -n "$1" ]
    then
        aim_address=$1
    else
        aim_address=$init_address
    fi
    N_case=$(find $aim_address -type f -name "OUTCAR" | wc -l)
    echo "当前文件夹内有$N_case个OUTCAR文件"
    if [ $N_case -eq 0 ]
    then
    echo "当前文件夹内有$N_case个OUTCAR文件"
    exit 1
    fi
    N_cout=0
    for outcar in $(find $aim_address -type f -name "OUTCAR" |sort);do
    all_atom=$(grep "number of ions" $outcar | tail -n 1 | awk '{ print $NF }')
    if [ -z "$all_atom" ];then
        continue
    fi

    if [ $? -ne 0 ]; then
            echo "Error processing OUTCAR file: $outcar" >> error_log.txt
            continue # 跳到下一个文件
        fi
    config_type=$(basename $(dirname $outcar))
    weight=1.0
    lattice=$(grep -A 7 "VOLUME and BASIS-vectors are now" $outcar | tail -n 3 | awk '{ print $1,$2,$3 }' | xargs)
    if [ -z "$lattice" ];then
        continue
    fi
    energy=$(grep  'free  energy   TOTEN' $outcar | awk -F "=" '{ print $NF }' |tail -n 1 | sed 's/ //g')
    virial=$(grep -A 20 "FORCE on cell =-STRESS" $outcar | grep "Total" | tail -n 1 | awk '{ print $2,$5,$7,$5,$3,$6,$7,$6,$4 }')
    echo "$all_atom" >> $writ_file
    if   [ -n "$virial" ]
    then
        echo "Config_type=$config_type Weight=$weight Lattice=\"$lattice\" Energy=$energy Virial=\"$virial\" Properties=species:S:1:pos:R:3:force:R:3" >> $writ_file
    else
        echo "Config_type=$config_type Weight=$weight Lattice=\"$lattice\" Energy=$energy Properties=species:S:1:pos:R:3:force:R:3" >> $writ_file
    fi
    element_str=$(grep "VRHFIN" $outcar | awk -F"=" '{print $2}' |awk -F ":" '{print $1}')
    ion_element_array=($element_str)
    ion_num_array=($(grep "ions per type"  $outcar | tail -n 1 | awk -F"=" '{print $2}'))
    
    for((i=0;i<${#ion_element_array[@]};i++))
    do
        for((j=0;j<${ion_num_array[i]};j++))
        do
            echo "${ion_element_array[$i]}" >> atom_name
        done
    done
    grep -A $((all_atom+1)) "TOTAL-FORCE (eV/Angst)" $outcar | tail -n $all_atom >> atom_pos
    paste atom_name atom_pos >> $writ_file
    rm atom_name atom_pos
    N_cout=$((N_cout+1))
    done
    echo "当前工作目录为$init_address"
    echo "训练集新增构型数目为 $N_cout" '让我们欢迎新增的构型>o<'
    echo "完成数目任务数与总任务之比 $N_cout/$N_case"
    endtime=$(date +%s)
    alltime=$((endtime-startime))
    min_time=$((alltime/60))
    echo -e "--------------------->完美完成工作,总共用时$min_time min OVO"
}
#+++++++++++++++++++++++++++文件操作相关工具+++++++++++++++++++++++
recc(){
#创建一个临时的文件来保存指定文件的地址，方便后续对其进行操作
    cc_tmpfile="/tmp/cc_fzmrkwjsbzz_pointer"
    for i in "$@"; do
        local fileaddress=${PWD}/$(basename $i)
        echo "$fileaddress" >> "$cc_tmpfile"
    done
}

revv(){
#粘贴之前的所有的记录下的文件，并且将其复制到当前的文件夹中
    local vv_dir=${1:-.}
    cat "$cc_tmpfile" | xargs -n 1  cp {} $PWD
    > "$cc_tmpfile"
}

function wslcd() {
#检查是否有windows路径，如果有就转换为wsl-linux路径
    local wsl_dir=$1
    if [[ $wsl_dir =~ ^([a-zA-Z]):(.*)$ ]]; then
        local drive=$(echo ${BASH_REMATCH[1]} | tr '[:upper:]' '[:lower:]')
        local path=${BASH_REMATCH[2]}
        path=$(echo $path | sed 's|\\|/|g')
        local wsl_path="/mnt/${drive}${path}"
        echo "转换后的wsl路径为：$wsl_path"
        cd  -- "$wsl_path"
    else
        cd  -- "$wsl_dir"
    fi
}

winpath(){
#将windows路径转换为linux路径
    local wsl_dir=$1
    if [[ $wsl_dir =~ ^([a-zA-Z]):(.*)$ ]]; then
        local drive=$(echo ${BASH_REMATCH[1]} | tr '[:upper:]' '[:lower:]')
        local path=${BASH_REMATCH[2]}
        path=$(echo $path | sed 's|\\|/|g')
        local wsl_path="/mnt/${drive}${path}"
        echo "转换后的wsl路径为：$wsl_path"
fi
}

zone_group_to_xyz(){
# idea from bessel
# 本脚本的分组方式为按照指定的区域来进行分组，可以将模型文件按照指定的区域分为区域内与区域外两组
# 使用方法为 zone_group_to_xyz POSCAR y 0 10 将会按照y方向的距离来进行分组，距离在0-10之间为组1,其他的为组0
    local files_num=$1
    local direction=$2
    local min_distance=$3
    local max_distance=$4
    python3 << EOF
import numpy as np
import ase.io
filename = '$files_num'
direction = '$direction'  
min_distance = $min_distance  
max_distance = $max_distance
xyzinfo = ase.io.read(filename)
cell_x = xyzinfo.cell[0][0]
cell_y = xyzinfo.cell[1][1]
cell_z = xyzinfo.cell[2][2]
positions = xyzinfo.get_positions()
match direction:
    case 'z':
        pos = positions[:, 2]
        cell_length = cell_z
    case 'y':
        pos = positions[:, 1]
        cell_length = cell_y
    case 'x':
        pos = positions[:, 0]
        cell_length = cell_x
    case _:
        print("Invalid direction")
        raise ValueError("Invalid direction specified.")
indices = (pos >= min_distance)&(pos <= max_distance)
xyzinfo.arrays['group'] = indices.astype(int)
ase.io.write('model_group.xyz', xyzinfo)
EOF
}

crystal_face_distance_grouping() {
# 本脚本的分组方式为按照晶面与晶面的距离来进行分组，可以将模型文件按照晶面与晶面的距离分为距离内与距离外两组
# 使用方法为 crystal_face_distance_grouping POSCAR [1,1,1] -2 2 将会按照原子距离晶面111面之间的距离进行分组，将POSCAR距离晶面111在-2，2之间为组1,其他的为组0
local filename=$1
local crystal_face=$2
local min_distant=$3
local maxdistant=$4
python3 << EOF
import numpy as np
import ase.io
filename = '$filename'
crystal_face = $crystal_face
min_distant = $min_distant
maxdistant = $maxdistant
xyzinfo = ase.io.read(filename)
cell_x = xyzinfo.cell[0][0]
cell_y = xyzinfo.cell[1][1]
cell_z = xyzinfo.cell[2][2]
positions = xyzinfo.get_positions()
pos_x = positions[:, 0]
pos_y = positions[:, 1]
pos_z = positions[:, 2]
def create_group(filename, crystal_face):
    A = crystal_face[0]
    B = crystal_face[1]
    C = crystal_face[2]
    D = -(crystal_face[0] * cell_x)
    numerator = (A*pos_x + B*pos_y + C*pos_z + D)
    denominator = np.sqrt(A**2 + B**2 + C**2)
    distance = numerator / denominator
    flag = np.logical_and(distance > min_distant, distance < maxdistant)
    group_flag = flag.astype(int)
    xyzinfo.arrays['group'] = group_flag
    ase.io.write('crystalface_grouping.xyz', xyzinfo)
create_group(filename, crystal_face)
EOF
}


#+++++++++++++++++++++++++++数学计算相关工具++++++++++++++++++++++++++

pow(){
#计算幂次方
    local base=$1
    local exp=$2
    echo $((base**exp))
}

calc_time(){
#计算运行时间
    local start_time=$(date +%s%3N)
    eval "$@"
    local end_time=$(date +%s%3N)

    local time_diff=$((end_time-start_time))
    echo "运行时间为： $time_diff 毫秒"
}

echo "NebulaFlow library loaded" 'O.<'

#-----------------------------模板-----------------------------------


compute_high_of_tree() {
# 创建一个临时文件夹来保存生成的文件
    local tmpdir=$(mktemp -d -t high_of_tree-XXXXXX)
    
    # 使用指定的临时文件夹
    pushd "$tmpdir"
    
    # 使用shell调用c++的模板
    cat > high_of_tree.cpp <<EOF
    #include <iostream>
    #include <cmath>
    #include <numbers>

    using namespace std;

    void high_of_tree(){
        const double pi  {std::numbers::pi};
        double high {};
        double h {};
        double d {};
        double angle {};
        cout << "计算树的高度" << endl;
        cout << "please enter h ,d ,angle：";
        
        cin >> h >> d >> angle;
        cout << "h = " << h << "  d = " << d << "  angle = " << angle << endl;
        cout << "树的高度是" << h + d * std::tan(angle * pi / 180) << endl;
    }

    int main(){
        high_of_tree();
        return 0;
    }
EOF

# 编译C++代码
g++ -std=c++20 high_of_tree.cpp -o high_of_tree

# 运行程序
./high_of_tree

# 从临时文件夹中返回到原始目录
popd

# 删除临时文件夹及其内容
rm -rf "$tmpdir"
}




#-----------------------------玩具-----------------------------------------------

gpt9() {
    # 开启智能对话
    echo "hallo, 我是gpt9, 你好"
    while true; do
        echo -en "\e[33m$USER: \e[0m"
        read -r line
        case "$line" in
            exit|Exit|EXIT)
                echo "gpt9：再见！"
                break
                ;;
            *)
                processed_line=$(echo "$line" | sed -e 's/吗//g; s/you//g; s/?/!/g; s/^你/我/g; s/我/你/g; s/吧//g; s/？/！/g')
                echo -e "\e[32mgpt9：\e[0m $processed_line\n"
                ;;
        esac
    done
}

mycat(){
# Ai时代,沸腾期待
     echo "喵喵喵?(被迫营业的叫声)"
     while true; do
         echo -en "\e[33m$USER: \e[0m"
         read -r line
         if [[ "$line" == "exit" ]]; then
             echo "(o^‥^)o mew！(不舍的叫声)"
             break
         fi

        emojis=('◟[˳_˳]ʌ˽ʌ' '(=♡ ᆺ ♡=)' '=＾• ⋏ •＾=' '̳ ៱˳_˳៱ ̳ ∫' '＾⌤＾' '[^._.^]ﾉ彡' '/ᐠ｡ꞈ｡ᐟ\' '/ᐠ｡ꞈ｡ᐟ✿\' '✧/ᐠ-ꞈ-ᐟ\' '/ᐠ𝅒 ‸ 𝅒ᐟ\ﾉ' '—ฅ/ᐠ. ̫ .ᐟ\ฅ —' '/ᐠ_ ꞈ _ᐟ\ɴʏᴀ~' 'ฅ^•ﻌ•^ฅ' '(ฅ^･ω･^ ฅ)' '(´ฅω•ฅ｀)' '~(=^‥^)ﾉ◎～')

        random_emoji=${emojis[$RANDOM % ${#emojis[@]} ]}
         random_number=$(shuf -i 1-4 -n 1)
         processed_line=$(printf "%0.s喵" $(seq 1 $random_number))
         echo -e "\e[32m${random_emoji}：\e[0m $processed_line\n"

     done
}